{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import shutil\n",
    "from pathlib import Path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def preprocess_emoji(dset_path):\n",
    "    # make emoji unicode vocabulary \n",
    "    code_vocaburary = {}\n",
    "    code_path = Path('dataset/description/unicode.txt')\n",
    "    code_list = code_path.read_text(encoding='utf-8').split('\\n')\n",
    "    for index, data in enumerate(code_list):\n",
    "        code_vocaburary[data] = index\n",
    "    \n",
    "    # chack dataset path\n",
    "    image_path = Path(dset_path)\n",
    "    if image_path.exists() == False:\n",
    "        exit('Check your dataset path!')\n",
    "\n",
    "    # copy designated emoji images\n",
    "    for filepath in list(image_path.glob(\"./**/64/**/*.png\")):\n",
    "        if str(filepath.name.split(\".\")[0]) in code_list:\n",
    "            shutil.copyfile(filepath, \\\n",
    "                'dataset/edited/' + str(code_vocaburary[filepath.name.split(\".\")[0]]) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started pre-processing\n",
      "Completed pre-processing\n"
     ]
    }
   ],
   "source": [
    "print(\"Started pre-processing\")\n",
    "preprocess_emoji(\"dataset/original\")\n",
    "print(\"Completed pre-processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of images we will be generating (and training upon)\n",
    "# 64 X 64 X 3(channels)\n",
    "IMAGE_SHAPE = (64, 64, 3)\n",
    "\n",
    "# dimension of word_vector (embedding) = 300 X  1\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "# directory containing preprocessed images for training\n",
    "IMAGE_DIR = \"dataset/edited/\"\n",
    "\n",
    "# directory containing preprocessed txt\n",
    "TXT_DIR = \"dataset/description/detailed/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# function to load dataset\n",
    "def load_dataset(img_dir, txt_dir, img_shape, split_rate = 0.1):\n",
    "    t_path = Path(txt_dir)\n",
    "    i_path = Path(img_dir)\n",
    "    \n",
    "    images = dict()\n",
    "    texts = dict()\n",
    "    \n",
    "    for filename in list(i_path.glob(\"*.png\")):\n",
    "        name = filename.name.replace('.png', '')\n",
    "        images[name] = filename.resolve()\n",
    "    \n",
    "    for filename in list(t_path.glob(\"*.txt\")):\n",
    "        name = filename.name.replace('.txt', '')\n",
    "        texts[name] = filename.read_text(encoding='utf-8').lower()\n",
    "\n",
    "    image_list = []\n",
    "    caption_list = []\n",
    "    numbers = []\n",
    "    \n",
    "    for name, item_path in images.items():\n",
    "        if name in texts:\n",
    "            text = texts[name]\n",
    "            text = text.replace(\"“\", \"\") # need to remove explicitly as ascii has only one double-quotes, no start-end double-quotes\n",
    "            text = text.replace(\"”\", \"\")\n",
    "            tokenized = sent_tokenize(text) # tokenizes sentences, delimiter = \".\"\n",
    "            label_number = int(name)\n",
    "\n",
    "            for sentence in tokenized:\n",
    "                regex_any_symbol = re.compile(\"[!-/:-@[-`{-~]\")\n",
    "                filtered_sentence = re.sub(regex_any_symbol, \"\", sentence) # removes any symbol from description\n",
    "#                 print(filtered_sentence)\n",
    "                image = img_to_array(load_img(item_path, target_size=(img_shape[0], img_shape[1])))\n",
    "                image = (image.astype(np.float32) / 127.5) - 1.\n",
    "#                 print(image)\n",
    "                image_list.append(image)\n",
    "                caption_list.append(filtered_sentence)\n",
    "                numbers.append(label_number)\n",
    "                \n",
    "    image_list = np.array(image_list)\n",
    "    caption_list = np.array(caption_list)\n",
    "    numbers = np.array(numbers)\n",
    "    \n",
    "    print('Dataset Size: %s' % len(image_list))\n",
    "    image_train, image_test, caption_train, caption_test, numbers_train, numbers_test = train_test_split(image_list, caption_list, numbers, test_size=split_rate)\n",
    "    \n",
    "    return image_train, caption_train, image_test, caption_test, numbers_train, numbers_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size: 260\n"
     ]
    }
   ],
   "source": [
    "# Loading dataset: \n",
    "image_train, caption_train, image_test, caption_test, numbers_train, numbers_test \\\n",
    "    = load_dataset(IMAGE_DIR,TXT_DIR, IMAGE_SHAPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_glove(glove_file_path, embedding_dim = EMBEDDING_DIM):\n",
    "    print(\"Loading glove file, please wait...\")\n",
    "    _word2em = {}\n",
    "    file = open(glove_file_path, mode='rt', encoding='utf8')\n",
    "    for line in file:\n",
    "        words = line.strip().split()\n",
    "        word = words[0]\n",
    "        embeds = np.array(words[1:], dtype=np.float32)\n",
    "        _word2em[word] = embeds\n",
    "    file.close()\n",
    "    print(\"Finished.\")\n",
    "    return _word2em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading glove file, please wait...\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "word2em = load_glove(\"/home/kaustubh/emotigan/utils/glove.6B.300d.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence2Em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# returns embedding for a sentence\n",
    "def vectorize_sentence(sentence, embedding_dim = EMBEDDING_DIM):\n",
    "    words = sentence.split(\" \")\n",
    "    em = np.zeros(shape=(embedding_dim, ))\n",
    "    for word in words:\n",
    "        try:\n",
    "            em = np.add(em, word2em[word])\n",
    "        except KeyError:\n",
    "            print(\"Error: Not found \\\"\"+word+\"\\\"\")\n",
    "    return em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Not found \"emoji\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing\n",
    "len(vectorize_sentence(caption_test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu_options {\n",
      "  allow_growth: true\n",
      "  visible_device_list: \"0\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GPU setting\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.backend import set_session\n",
    "\n",
    "config = tf.ConfigProto(\n",
    "            gpu_options = tf.GPUOptions(\n",
    "                visible_device_list=\"0\", # specify GPU number\n",
    "                allow_growth=True)\n",
    "        )\n",
    "\n",
    "print(config)\n",
    "\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, concatenate\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as kb\n",
    "from keras.layers import Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return generator model (keras)\n",
    "def build_generator(latent_dim = 100, embedding_dim = EMBEDDING_DIM, channels = IMAGE_SHAPE[2]):\n",
    "    generator_input = Input(shape=(latent_dim, ), name=\"g_input\")\n",
    "    cond_input = Input(shape=(embedding_dim, ), name=\"cond_g_input\")\n",
    "    cond_output = Dense(100)(cond_input)\n",
    "\n",
    "    G = concatenate([generator_input, cond_output])\n",
    "    G = Dense(256 * 8 * 8, activation=\"relu\")(G)\n",
    "    G = Reshape((8, 8, 256))(G)\n",
    "    G = UpSampling2D()(G)\n",
    "    G = Conv2D(256, kernel_size=3, padding=\"same\")(G)\n",
    "    G = BatchNormalization(momentum=0.8)(G)\n",
    "    G = Activation(\"relu\")(G)\n",
    "    G = UpSampling2D()(G)\n",
    "    G = Conv2D(128, kernel_size=3, padding=\"same\")(G)\n",
    "    G = BatchNormalization(momentum=0.8)(G)\n",
    "    G = Activation(\"relu\")(G)\n",
    "    G = UpSampling2D()(G)\n",
    "    G = Conv2D(64, kernel_size=3, padding=\"same\")(G)\n",
    "    G = BatchNormalization(momentum=0.8)(G)\n",
    "    G = Activation(\"relu\")(G)\n",
    "    G = Conv2D(filters = channels, kernel_size=3, padding=\"same\")(G)\n",
    "    G = Activation(\"tanh\")(G)\n",
    "\n",
    "    generator = Model([generator_input, cond_input], G)\n",
    "    generator.summary()\n",
    "\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(embedding_dim = EMBEDDING_DIM, img_shape = IMAGE_SHAPE):\n",
    "        discriminator_input = Input(shape=img_shape, name=\"d_input\")\n",
    "        cond_input = Input(shape=(embedding_dim, ), name=\"cond_d_input\")\n",
    "        \n",
    "        D = Conv2D(64, kernel_size=3, strides=2, padding=\"same\")(discriminator_input)\n",
    "        D = LeakyReLU(alpha=0.2)(D)\n",
    "        D = Dropout(0.25)(D)\n",
    "        D = Conv2D(128, kernel_size=3, strides=2, padding=\"same\")(D)\n",
    "        D = ZeroPadding2D(padding=((0,1),(0,1)))(D)\n",
    "        D = BatchNormalization(momentum=0.8)(D)\n",
    "        D = LeakyReLU(alpha=0.2)(D)\n",
    "        D = Dropout(0.25)(D)\n",
    "        D = Conv2D(256, kernel_size=3, strides=1, padding=\"same\")(D)\n",
    "        D = BatchNormalization(momentum=0.8)(D)\n",
    "        D = LeakyReLU(alpha=0.2)(D)\n",
    "        D = Dropout(0.25)(D)\n",
    "        D = Conv2D(512, kernel_size=3, strides=2, padding=\"same\")(D)\n",
    "        D = BatchNormalization(momentum=0.8)(D)\n",
    "        D = LeakyReLU(alpha=0.2)(D)\n",
    "\n",
    "        cond_d_hidden = Dense(100)(cond_input)\n",
    "        cond_d_hidden = Reshape((1, 1, 100))(cond_d_hidden)\n",
    "        cond_d_output = Lambda(lambda x: kb.tile(x, [1, 9, 9, 1]))(cond_d_hidden)\n",
    "\n",
    "        D = concatenate([D, cond_d_output], axis=-1)\n",
    "        D = Conv2D(512, kernel_size=3, strides=1, padding='same')(D)\n",
    "        D = BatchNormalization(momentum=0.8)(D)\n",
    "        D = LeakyReLU(alpha=0.1)(D)\n",
    "        D = Dropout(0.25)(D)\n",
    "        D = Flatten()(D)\n",
    "        discriminator_output = Dense(1, activation='sigmoid')(D)\n",
    "\n",
    "        discriminator = Model([discriminator_input, cond_input], discriminator_output)\n",
    "        discriminator.summary()\n",
    "\n",
    "        return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizers\n",
    "optimizer_g = Adam(0.0005, 0.5)\n",
    "optimizer_d = Adam(0.00005, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "cond_g_input (InputLayer)       (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "g_input (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 100)          30100       cond_g_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 200)          0           g_input[0][0]                    \n",
      "                                                                 dense_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 16384)        3293184     concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_15 (Reshape)            (None, 8, 8, 256)    0           dense_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_40 (UpSampling2D) (None, 16, 16, 256)  0           reshape_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 16, 16, 256)  590080      up_sampling2d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 16, 16, 256)  1024        conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 16, 16, 256)  0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_41 (UpSampling2D) (None, 32, 32, 256)  0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 32, 32, 128)  295040      up_sampling2d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 32, 32, 128)  512         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 32, 32, 128)  0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_42 (UpSampling2D) (None, 64, 64, 128)  0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 64, 64, 64)   73792       up_sampling2d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 64, 64, 64)   256         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 64, 64, 64)   0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 64, 64, 3)    1731        activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 64, 64, 3)    0           conv2d_55[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,285,719\n",
      "Trainable params: 4,284,823\n",
      "Non-trainable params: 896\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the generator\n",
    "generator = build_generator()\n",
    "\n",
    "latent_dim = 100\n",
    "\n",
    "# The generator takes noise as input and generates imgs\n",
    "z = Input(shape=(latent_dim,))\n",
    "cond_input = Input(shape=(EMBEDDING_DIM,))\n",
    "img = generator([z, cond_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "d_input (InputLayer)            (None, 64, 64, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 32, 32, 64)   1792        d_input[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 32, 32, 64)   0           conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 32, 32, 64)   0           leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 16, 16, 128)  73856       dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPadding2D (None, 17, 17, 128)  0           conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 17, 17, 128)  512         zero_padding2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 17, 17, 128)  0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 17, 17, 128)  0           leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 17, 17, 256)  295168      dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 17, 17, 256)  1024        conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 17, 17, 256)  0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 17, 17, 256)  0           leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "cond_d_input (InputLayer)       (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 9, 9, 512)    1180160     dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 100)          30100       cond_d_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 9, 9, 512)    2048        conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_16 (Reshape)            (None, 1, 1, 100)    0           dense_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 9, 9, 512)    0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 9, 9, 100)    0           reshape_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 9, 9, 612)    0           leaky_re_lu_9[0][0]              \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 9, 9, 512)    2820608     concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 9, 9, 512)    2048        conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 9, 9, 512)    0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 9, 9, 512)    0           leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 41472)        0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 1)            41473       flatten_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,448,789\n",
      "Trainable params: 4,445,973\n",
      "Non-trainable params: 2,816\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build and Compile Discriminator\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer_d,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "# For the combined model we will only train the generator\n",
    "discriminator.trainable = False\n",
    "\n",
    "# The discriminator takes generated images as input and determines validity\n",
    "valid = discriminator([img, cond_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The combined model  (stacked generator and discriminator)\n",
    "# Trains the generator to fool the discriminator\n",
    "combined = Model([z, cond_input], valid)\n",
    "combined.compile(loss='binary_crossentropy', optimizer=optimizer_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
