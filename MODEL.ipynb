{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import shutil\n",
    "from pathlib import Path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_emoji(dset_path):\n",
    "    # make emoji unicode vocabulary \n",
    "    code_vocaburary = {}\n",
    "    code_path = Path('dataset/description/unicode.txt')\n",
    "    code_list = code_path.read_text(encoding='utf-8').split('\\n')\n",
    "    for index, data in enumerate(code_list):\n",
    "        code_vocaburary[data] = index\n",
    "    \n",
    "    # chack dataset path\n",
    "    image_path = Path(dset_path)\n",
    "    if image_path.exists() == False:\n",
    "        exit('Check your dataset path!')\n",
    "\n",
    "    # copy designated emoji images\n",
    "    for filepath in list(image_path.glob(\"./**/64/**/*.png\")):\n",
    "        if str(filepath.name.split(\".\")[0]) in code_list:\n",
    "            shutil.copyfile(filepath, \\\n",
    "                'dataset/edited/' + str(code_vocaburary[filepath.name.split(\".\")[0]]) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started pre-processing\n",
      "Completed pre-processing\n"
     ]
    }
   ],
   "source": [
    "DATASET_LOCATION = \"dataset/original\"\n",
    "print(\"Started pre-processing\")\n",
    "preprocess_emoji(DATASET_LOCATION)\n",
    "print(\"Completed pre-processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu_options {\n",
      "  allow_growth: true\n",
      "  visible_device_list: \"0\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GPU setting\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.backend import set_session\n",
    "\n",
    "config = tf.ConfigProto(\n",
    "            gpu_options = tf.GPUOptions(\n",
    "                visible_device_list=\"0\", # specify GPU number\n",
    "                allow_growth=True)\n",
    "        )\n",
    "\n",
    "print(config)\n",
    "\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils.glove_loader'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3bb9e4ecfae4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mkb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglove_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGloveModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Agg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils.glove_loader'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, concatenate\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as kb\n",
    "from keras.layers import Lambda\n",
    "from utils.glove_loader import GloveModel\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from utils.dataset_utils import load_dataset\n",
    "from PIL import Image\n",
    "import math\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN():\n",
    "    def __init__(self, img_path, txt_path, glove_path):\n",
    "        # Input shape\n",
    "        self.img_rows = 64\n",
    "        self.img_cols = 64\n",
    "        self.channels = 3\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "        self.embedding_dim = 300\n",
    "        self.img_path = img_path\n",
    "        self.txt_path = txt_path\n",
    "        self.glove_path = glove_path\n",
    "\n",
    "        optimizer_g = Adam(0.0005, 0.5)\n",
    "        optimizer_d = Adam(0.00005, 0.5)\n",
    "\n",
    "        # Build the GloVe model\n",
    "        self.glove_model = GloveModel()\n",
    "        self.glove_model.load(data_dir_path=self.glove_path, embedding_dim=self.embedding_dim)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer_d,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        cond_input = Input(shape=(self.embedding_dim,))\n",
    "        img = self.generator([z, cond_input])\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        valid = self.discriminator([img, cond_input])\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model([z, cond_input], valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer_g)\n",
    "\n",
    "    def build_generator(self):\n",
    "        generator_input = Input(shape=(self.latent_dim, ), name=\"g_input\")\n",
    "        cond_input = Input(shape=(self.embedding_dim, ), name=\"cond_g_input\")\n",
    "        cond_output = Dense(100)(cond_input)\n",
    "\n",
    "        G = concatenate([generator_input, cond_output])\n",
    "        G = Dense(256 * 8 * 8, activation=\"relu\")(G)\n",
    "        G = Reshape((8, 8, 256))(G)\n",
    "        G = UpSampling2D()(G)\n",
    "        G = Conv2D(256, kernel_size=3, padding=\"same\")(G)\n",
    "        G = BatchNormalization(momentum=0.8)(G)\n",
    "        G = Activation(\"relu\")(G)\n",
    "        G = UpSampling2D()(G)\n",
    "        G = Conv2D(128, kernel_size=3, padding=\"same\")(G)\n",
    "        G = BatchNormalization(momentum=0.8)(G)\n",
    "        G = Activation(\"relu\")(G)\n",
    "        G = UpSampling2D()(G)\n",
    "        G = Conv2D(64, kernel_size=3, padding=\"same\")(G)\n",
    "        G = BatchNormalization(momentum=0.8)(G)\n",
    "        G = Activation(\"relu\")(G)\n",
    "        G = Conv2D(self.channels, kernel_size=3, padding=\"same\")(G)\n",
    "        generator_output = Activation(\"tanh\")(G)\n",
    "\n",
    "        generator = Model([generator_input, cond_input], generator_output)\n",
    "        generator.summary()\n",
    "\n",
    "        return generator\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        discriminator_input = Input(shape=self.img_shape, name=\"d_input\")\n",
    "        cond_input = Input(shape=(self.embedding_dim, ), name=\"cond_d_input\")\n",
    "        D = Conv2D(64, kernel_size=3, strides=2, padding=\"same\")(discriminator_input)\n",
    "        D = LeakyReLU(alpha=0.2)(D)\n",
    "        D = Dropout(0.25)(D)\n",
    "        D = Conv2D(128, kernel_size=3, strides=2, padding=\"same\")(D)\n",
    "        D = ZeroPadding2D(padding=((0,1),(0,1)))(D)\n",
    "        D = BatchNormalization(momentum=0.8)(D)\n",
    "        D = LeakyReLU(alpha=0.2)(D)\n",
    "        D = Dropout(0.25)(D)\n",
    "        D = Conv2D(256, kernel_size=3, strides=1, padding=\"same\")(D)\n",
    "        D = BatchNormalization(momentum=0.8)(D)\n",
    "        D = LeakyReLU(alpha=0.2)(D)\n",
    "        D = Dropout(0.25)(D)\n",
    "        D = Conv2D(512, kernel_size=3, strides=2, padding=\"same\")(D)\n",
    "        D = BatchNormalization(momentum=0.8)(D)\n",
    "        D = LeakyReLU(alpha=0.2)(D)\n",
    "\n",
    "        cond_d_hidden = Dense(100)(cond_input)\n",
    "        cond_d_hidden = Reshape((1, 1, 100))(cond_d_hidden)\n",
    "        cond_d_output = Lambda(lambda x: kb.tile(x, [1, 9, 9, 1]))(cond_d_hidden)\n",
    "\n",
    "        D = concatenate([D, cond_d_output], axis=-1)\n",
    "        D = Conv2D(512, kernel_size=3, strides=1, padding='same')(D)\n",
    "        D = BatchNormalization(momentum=0.8)(D)\n",
    "        D = LeakyReLU(alpha=0.1)(D)\n",
    "        D = Dropout(0.25)(D)\n",
    "        D = Flatten()(D)\n",
    "        discriminator_output = Dense(1, activation='sigmoid')(D)\n",
    "\n",
    "        discriminator = Model([discriminator_input, cond_input], discriminator_output)\n",
    "        discriminator.summary()\n",
    "\n",
    "        return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
