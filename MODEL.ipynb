{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import shutil\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def preprocess_emoji(dset_path):\n",
    "    # make emoji unicode vocabulary\n",
    "    code_vocaburary = {}\n",
    "    code_path = Path(\"dataset/description/unicode.txt\")\n",
    "    code_list = code_path.read_text(encoding=\"utf-8\").split(\"\\n\")\n",
    "    for index, data in enumerate(code_list):\n",
    "        code_vocaburary[data] = index\n",
    "\n",
    "    # chack dataset path\n",
    "    image_path = Path(dset_path)\n",
    "    if image_path.exists() == False:\n",
    "        exit(\"Check your dataset path!\")\n",
    "\n",
    "    # copy designated emoji images\n",
    "    for filepath in list(image_path.glob(\"./**/64/**/*.png\")):\n",
    "        if str(filepath.name.split(\".\")[0]) in code_list:\n",
    "            shutil.copyfile(\n",
    "                filepath,\n",
    "                \"dataset/edited/\"\n",
    "                + str(code_vocaburary[filepath.name.split(\".\")[0]])\n",
    "                + \".png\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started pre-processing\n",
      "Completed pre-processing\n"
     ]
    }
   ],
   "source": [
    "print(\"Started pre-processing\")\n",
    "preprocess_emoji(\"dataset/original\")\n",
    "print(\"Completed pre-processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of images we will be generating (and training upon)\n",
    "# 64 X 64 X 3(channels)\n",
    "IMAGE_SHAPE = (64, 64, 3)\n",
    "\n",
    "# dimension of word_vector (embedding) = 300 X  1\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "# Latent dim used in layers of models = 100  X 1\n",
    "LATENT_DIM = 100\n",
    "\n",
    "# directory containing preprocessed images for training\n",
    "IMAGE_DIR = \"dataset/edited/\"\n",
    "\n",
    "# directory containing preprocessed txt\n",
    "TXT_DIR = \"dataset/description/detailed/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# function to load dataset\n",
    "def load_dataset(img_dir, txt_dir, img_shape, split_rate=0.1):\n",
    "    t_path = Path(txt_dir)\n",
    "    i_path = Path(img_dir)\n",
    "\n",
    "    images = dict()\n",
    "    texts = dict()\n",
    "\n",
    "    for filename in list(i_path.glob(\"*.png\")):\n",
    "        name = filename.name.replace(\".png\", \"\")\n",
    "        images[name] = filename.resolve()\n",
    "\n",
    "    for filename in list(t_path.glob(\"*.txt\")):\n",
    "        name = filename.name.replace(\".txt\", \"\")\n",
    "        texts[name] = filename.read_text(encoding=\"utf-8\").lower()\n",
    "\n",
    "    image_list = []\n",
    "    caption_list = []\n",
    "    numbers = []\n",
    "\n",
    "    for name, item_path in images.items():\n",
    "        if name in texts:\n",
    "            text = texts[name]\n",
    "            text = text.replace(\n",
    "                \"“\", \"\"\n",
    "            )  # need to remove explicitly as ascii has only one double-quotes, no start-end double-quotes\n",
    "            text = text.replace(\"”\", \"\")\n",
    "            tokenized = sent_tokenize(text)  # tokenizes sentences, delimiter = \".\"\n",
    "            label_number = int(name)\n",
    "\n",
    "            for sentence in tokenized:\n",
    "                regex_any_symbol = re.compile(\"[!-/:-@[-`{-~]\")\n",
    "                filtered_sentence = re.sub(\n",
    "                    regex_any_symbol, \"\", sentence\n",
    "                )  # removes any symbol from description\n",
    "                #                 print(filtered_sentence)\n",
    "                image = img_to_array(\n",
    "                    load_img(item_path, target_size=(img_shape[0], img_shape[1]))\n",
    "                )\n",
    "                image = (image.astype(np.float32) / 127.5) - 1.0\n",
    "                #                 print(image)\n",
    "                image_list.append(image)\n",
    "                caption_list.append(filtered_sentence)\n",
    "                numbers.append(label_number)\n",
    "\n",
    "    image_list = np.array(image_list)\n",
    "    caption_list = np.array(caption_list)\n",
    "    numbers = np.array(numbers)\n",
    "\n",
    "    print(\"Dataset Size: %s\" % len(image_list))\n",
    "    (\n",
    "        image_train,\n",
    "        image_test,\n",
    "        caption_train,\n",
    "        caption_test,\n",
    "        numbers_train,\n",
    "        numbers_test,\n",
    "    ) = train_test_split(image_list, caption_list, numbers, test_size=split_rate)\n",
    "\n",
    "    return (\n",
    "        image_train,\n",
    "        caption_train,\n",
    "        image_test,\n",
    "        caption_test,\n",
    "        numbers_train,\n",
    "        numbers_test,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size: 260\n"
     ]
    }
   ],
   "source": [
    "# Loading dataset:\n",
    "(\n",
    "    image_train,\n",
    "    caption_train,\n",
    "    image_test,\n",
    "    caption_test,\n",
    "    numbers_train,\n",
    "    numbers_test,\n",
    ") = load_dataset(IMAGE_DIR, TXT_DIR, IMAGE_SHAPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_glove(glove_file_path, embedding_dim=EMBEDDING_DIM):\n",
    "    print(\"Loading glove file, please wait...\")\n",
    "    _word2em = {}\n",
    "    file = open(glove_file_path, mode=\"rt\", encoding=\"utf8\")\n",
    "    for line in file:\n",
    "        words = line.strip().split()\n",
    "        word = words[0]\n",
    "        embeds = np.array(words[1:], dtype=np.float32)\n",
    "        _word2em[word] = embeds\n",
    "    file.close()\n",
    "    print(\"Finished.\")\n",
    "    return _word2em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading glove file, please wait...\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "word2em = load_glove(\"/home/kaustubh/emotigan/utils/glove.6B.300d.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence2Em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# returns embedding<output> for a sentence<input>\n",
    "def vectorize_sentence(sentence, embedding_dim=EMBEDDING_DIM):\n",
    "    words = sentence.split(\" \")\n",
    "    em = np.zeros(shape=(embedding_dim,))\n",
    "    for word in words:\n",
    "        try:\n",
    "            em = np.add(em, word2em[word])\n",
    "        except KeyError:\n",
    "            #             print('Error: Not found \"' + word + '\"')\n",
    "            pass\n",
    "    return em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize list of sentences\n",
    "def vectorize_sent_list(sent_list):\n",
    "    out = []\n",
    "    for sent in sent_list:\n",
    "        v = vectorize_sentence(sent)\n",
    "        out.append(v)\n",
    "    return np.array(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu_options {\n",
      "  allow_growth: true\n",
      "  visible_device_list: \"0\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GPU setting\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.backend import set_session\n",
    "\n",
    "config = tf.ConfigProto(\n",
    "    gpu_options=tf.GPUOptions(\n",
    "        visible_device_list=\"0\", allow_growth=True  # specify GPU number\n",
    "    )\n",
    ")\n",
    "\n",
    "print(config)\n",
    "\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, concatenate\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as kb\n",
    "from keras.layers import Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return generator model (keras)\n",
    "def build_generator(\n",
    "    latent_dim=LATENT_DIM, embedding_dim=EMBEDDING_DIM, channels=IMAGE_SHAPE[2]\n",
    "):\n",
    "    generator_input = Input(shape=(latent_dim,), name=\"g_input\")\n",
    "    cond_input = Input(shape=(embedding_dim,), name=\"cond_g_input\")\n",
    "    cond_output = Dense(100)(cond_input)\n",
    "\n",
    "    G = concatenate([generator_input, cond_output])\n",
    "    G = Dense(256 * 8 * 8, activation=\"relu\")(G)\n",
    "    G = Reshape((8, 8, 256))(G)\n",
    "    G = UpSampling2D()(G)\n",
    "    G = Conv2D(256, kernel_size=3, padding=\"same\")(G)\n",
    "    G = BatchNormalization(momentum=0.8)(G)\n",
    "    G = Activation(\"relu\")(G)\n",
    "    G = UpSampling2D()(G)\n",
    "    G = Conv2D(128, kernel_size=3, padding=\"same\")(G)\n",
    "    G = BatchNormalization(momentum=0.8)(G)\n",
    "    G = Activation(\"relu\")(G)\n",
    "    G = UpSampling2D()(G)\n",
    "    G = Conv2D(64, kernel_size=3, padding=\"same\")(G)\n",
    "    G = BatchNormalization(momentum=0.8)(G)\n",
    "    G = Activation(\"relu\")(G)\n",
    "    G = Conv2D(filters=channels, kernel_size=3, padding=\"same\")(G)\n",
    "    G = Activation(\"tanh\")(G)\n",
    "\n",
    "    generator = Model([generator_input, cond_input], G)\n",
    "    generator.summary()\n",
    "\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(embedding_dim=EMBEDDING_DIM, img_shape=IMAGE_SHAPE):\n",
    "    discriminator_input = Input(shape=img_shape, name=\"d_input\")\n",
    "    cond_input = Input(shape=(embedding_dim,), name=\"cond_d_input\")\n",
    "\n",
    "    D = Conv2D(64, kernel_size=3, strides=2, padding=\"same\")(discriminator_input)\n",
    "    D = LeakyReLU(alpha=0.2)(D)\n",
    "    D = Dropout(0.25)(D)\n",
    "    D = Conv2D(128, kernel_size=3, strides=2, padding=\"same\")(D)\n",
    "    D = ZeroPadding2D(padding=((0, 1), (0, 1)))(D)\n",
    "    D = BatchNormalization(momentum=0.8)(D)\n",
    "    D = LeakyReLU(alpha=0.2)(D)\n",
    "    D = Dropout(0.25)(D)\n",
    "    D = Conv2D(256, kernel_size=3, strides=1, padding=\"same\")(D)\n",
    "    D = BatchNormalization(momentum=0.8)(D)\n",
    "    D = LeakyReLU(alpha=0.2)(D)\n",
    "    D = Dropout(0.25)(D)\n",
    "    D = Conv2D(512, kernel_size=3, strides=2, padding=\"same\")(D)\n",
    "    D = BatchNormalization(momentum=0.8)(D)\n",
    "    D = LeakyReLU(alpha=0.2)(D)\n",
    "\n",
    "    #     print(D._keras_shape)\n",
    "\n",
    "    cond_d_hidden = Dense(100)(cond_input)\n",
    "    cond_d_hidden = Reshape((1, 1, 100))(cond_d_hidden)\n",
    "\n",
    "    #     print(cond_d_hidden._keras_shape)\n",
    "\n",
    "    cond_d_output = Lambda(\n",
    "        lambda x: kb.tile(x, [1, 9, 9, 1]), output_shape=[9, 9, 100]\n",
    "    )(cond_d_hidden)\n",
    "\n",
    "    #     print(cond_d_output._keras_shape)\n",
    "\n",
    "    #         cond_d_output = Lambda(lambda x: kb.tile(x, [2,2,1]))(cond_d_hidden)\n",
    "\n",
    "    D = concatenate([D, cond_d_output], axis=-1)\n",
    "    D = Conv2D(512, kernel_size=3, strides=1, padding=\"same\")(D)\n",
    "    D = BatchNormalization(momentum=0.8)(D)\n",
    "    D = LeakyReLU(alpha=0.1)(D)\n",
    "    D = Dropout(0.25)(D)\n",
    "    D = Flatten()(D)\n",
    "    discriminator_output = Dense(1, activation=\"sigmoid\")(D)\n",
    "\n",
    "    discriminator = Model([discriminator_input, cond_input], discriminator_output)\n",
    "    discriminator.summary()\n",
    "\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizers\n",
    "optimizer_g = Adam(0.0005, 0.5)\n",
    "optimizer_d = Adam(0.00005, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "cond_g_input (InputLayer)       (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "g_input (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 100)          30100       cond_g_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 200)          0           g_input[0][0]                    \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16384)        3293184     concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 8, 8, 256)    0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2D)  (None, 16, 16, 256)  0           reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 256)  590080      up_sampling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 256)  1024        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 16, 16, 256)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2D)  (None, 32, 32, 256)  0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 128)  295040      up_sampling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 32, 32, 128)  512         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 128)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2D)  (None, 64, 64, 128)  0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 64, 64, 64)   73792       up_sampling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 64, 64, 64)   256         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 64, 64, 64)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 64, 64, 3)    1731        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 64, 64, 3)    0           conv2d_13[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,285,719\n",
      "Trainable params: 4,284,823\n",
      "Non-trainable params: 896\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the generator\n",
    "generator = build_generator()\n",
    "\n",
    "# The generator takes noise as input and generates imgs\n",
    "z = Input(shape=(LATENT_DIM,))\n",
    "cond_input = Input(shape=(EMBEDDING_DIM,))\n",
    "img = generator([z, cond_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "d_input (InputLayer)            (None, 64, 64, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 32, 32, 64)   1792        d_input[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 32, 32, 64)   0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 32, 32, 64)   0           leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 16, 16, 128)  73856       dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPadding2D (None, 17, 17, 128)  0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 17, 17, 128)  512         zero_padding2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 17, 17, 128)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 17, 17, 128)  0           leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 17, 17, 256)  295168      dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 17, 17, 256)  1024        conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 17, 17, 256)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 17, 17, 256)  0           leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "cond_d_input (InputLayer)       (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 9, 9, 512)    1180160     dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 100)          30100       cond_d_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 9, 9, 512)    2048        conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 1, 1, 100)    0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 9, 9, 512)    0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 9, 9, 100)    0           reshape_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 9, 9, 612)    0           leaky_re_lu_9[0][0]              \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 9, 9, 512)    2820608     concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 9, 9, 512)    2048        conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 9, 9, 512)    0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 9, 9, 512)    0           leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 41472)        0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            41473       flatten_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,448,789\n",
      "Trainable params: 4,445,973\n",
      "Non-trainable params: 2,816\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build and Compile Discriminator\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(\n",
    "    loss=\"binary_crossentropy\", optimizer=optimizer_d, metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# For the combined model we will only train the generator\n",
    "discriminator.trainable = False\n",
    "\n",
    "# The discriminator takes generated images as input and determines validity\n",
    "valid = discriminator([img, cond_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The combined model  (stacked generator and discriminator)\n",
    "# Trains the generator to fool the discriminator\n",
    "combined = Model([z, cond_input], valid)\n",
    "combined.compile(loss=\"binary_crossentropy\", optimizer=optimizer_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epochs and Batch Size\n",
    "EPOCHS = 5000\n",
    "# BATCH_SIZE = 26\n",
    "BATCH_SIZE = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "def train(epochs=EPOCHS, batch_size=BATCH_SIZE, save_interval=20, latent_dim=LATENT_DIM):\n",
    "    # load dataset\n",
    "    (X_train, Captions, X_test, Captions_test, Labels) = (\n",
    "        image_train,\n",
    "        caption_train,\n",
    "        image_test,\n",
    "        caption_test,\n",
    "        numbers_train,\n",
    "    )\n",
    "\n",
    "    caption_list_train = []\n",
    "    caption_list_test = []\n",
    "    \n",
    "    for caption in Captions:\n",
    "        caption_list_train.append([str(caption)])\n",
    "    for caption in Captions_test:\n",
    "        caption_list_test.append([str(caption)])\n",
    "        \n",
    "    df = pd.DataFrame(caption_list_train, columns=[\"caption\"])\n",
    "    df.to_csv(\"./saved_model/caption_train.csv\")\n",
    "    df = pd.DataFrame(caption_list_test, columns=[\"caption\"])\n",
    "    df.to_csv(\"./saved_model/caption_test.csv\")\n",
    "\n",
    "    # Adversarial ground truths\n",
    "    valid = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "\n",
    "    batch_count = int(X_train.shape[0] / batch_size)\n",
    "    history = []\n",
    "    history_test = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch_index in range(batch_count):\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half of images\n",
    "            # idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[batch_index * batch_size : (batch_index + 1) * batch_size]\n",
    "            texts_input = Captions[\n",
    "                batch_index * batch_size : (batch_index + 1) * batch_size\n",
    "            ]\n",
    "            \n",
    "            texts = vectorize_sent_list(texts_input)\n",
    "\n",
    "            # Sample noise and generate a batch of new images\n",
    "            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "            gen_imgs = generator.predict([noise, texts])\n",
    "\n",
    "            # Train the discriminator (real classified as ones and generated as zeros)\n",
    "            start = time.time()\n",
    "            d_loss_real = discriminator.train_on_batch([imgs, texts], valid)\n",
    "            d_loss_fake = discriminator.train_on_batch([gen_imgs, texts], fake)\n",
    "            batch_time_d = time.time() - start\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator (wants discriminator to mistake images as real)\n",
    "            start = time.time()\n",
    "            g_loss = combined.train_on_batch([noise, texts], valid)\n",
    "            batch_time_g = time.time() - start\n",
    "\n",
    "            # Plot the progress\n",
    "            batch_time = batch_time_d + batch_time_g\n",
    "            print(\n",
    "                \"%d-%d [D loss: %f, acc.: %.2f%%] [G loss: %f] [Time: %f]\"\n",
    "                % (epoch, batch_index, d_loss[0], 100 * d_loss[1], g_loss, batch_time)\n",
    "            )\n",
    "            history.append(\n",
    "                [epoch, batch_index, d_loss[0], 100 * d_loss[1], g_loss, batch_time]\n",
    "            )\n",
    "\n",
    "        # Test the model\n",
    "        texts_test = vectorize_sent_list(Captions_test)\n",
    "        noise_test = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        gen_imgs_test = generator.predict([noise_test, texts_test])\n",
    "        start = time.time()\n",
    "        d_loss_real_test = discriminator.test_on_batch([X_test, texts_test], valid)\n",
    "        d_loss_fake_test = discriminator.test_on_batch(\n",
    "            [gen_imgs_test, texts_test], fake\n",
    "        )\n",
    "        batch_time_d_test = time.time() - start\n",
    "        d_loss_test = 0.5 * np.add(d_loss_real_test, d_loss_fake_test)\n",
    "        start = time.time()\n",
    "        g_loss_test = combined.test_on_batch([noise_test, texts_test], valid)\n",
    "        batch_time_g_test = time.time() - start\n",
    "\n",
    "        # Plot the test progress\n",
    "        batch_time_test = batch_time_d_test + batch_time_g_test\n",
    "        print(\n",
    "            \"%d (test) [D loss: %f, acc.: %.2f%%] [G loss: %f] [Time: %f]\"\n",
    "            % (\n",
    "                epoch,\n",
    "                d_loss_test[0],\n",
    "                100 * d_loss_test[1],\n",
    "                g_loss_test,\n",
    "                batch_time_test,\n",
    "            )\n",
    "        )\n",
    "        history_test.append(\n",
    "            [epoch, d_loss_test[0], 100 * d_loss_test[1], g_loss_test, batch_time_test]\n",
    "        )\n",
    "\n",
    "        # If at save interval => save generated image samples & training weights\n",
    "        if epoch % save_interval == 0:\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            texts_input = Captions[idx]\n",
    "            texts = vectorize_sent_list(texts_input)\n",
    "            save_imgs(epoch, texts)\n",
    "\n",
    "            generator.save_weights(\n",
    "                filepath=\"./saved_model/generator_weights_\" + str(epoch) + \".h5\"\n",
    "            )\n",
    "            discriminator.save_weights(\n",
    "                filepath=\"./saved_model/discriminator_weights_\" + str(epoch) + \".h5\"\n",
    "            )\n",
    "\n",
    "    # save weights & history\n",
    "    df_train = pd.DataFrame(\n",
    "        history, columns=[\"epoch\", \"batch\", \"d_loss\", \"acc\", \"g_loss\", \"time[sec]\"]\n",
    "    )\n",
    "    df_train.to_csv(\"./saved_model/history.csv\")\n",
    "    df_test = pd.DataFrame(\n",
    "        history_test, columns=[\"epoch\", \"d_loss\", \"acc\", \"g_loss\", \"time[sec]\"]\n",
    "    )\n",
    "    df_test.to_csv(\"./saved_model/history_test.csv\")\n",
    "    generator.save_weights(filepath=\"./saved_model/generator_weights.h5\")\n",
    "    discriminator.save_weights(filepath=\"./saved_model/discriminator_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_imgs(epoch, texts, batch_size=BATCH_SIZE, latent_dim = LATENT_DIM):\n",
    "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "    if batch_size == 260:\n",
    "        texts = vectorize_sent_list(texts)\n",
    "    gen_imgs = generator.predict([noise, texts])\n",
    "    gen_img = combine_normalized_images(gen_imgs)\n",
    "    img_from_normalized_img(gen_img).save(\"images/snapshot/%d.png\" % epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(\n",
    "    gen_path=\"./saved_model/generator_weights.h5\",\n",
    "    dis_path=\"./saved_model/discriminator_weights.h5\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Function: load_model  \n",
    "    This function loads a pre-trained model.  \n",
    "\n",
    "    Input: model_dir_path: designate where weights file is.  \n",
    "    Output: None (pre-trained model will be loaded.)\n",
    "    \"\"\"\n",
    "\n",
    "    ### load weights\n",
    "    generator.load_weights(gen_path)\n",
    "    discriminator.load_weights(dis_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_from_text(text, flag=True):\n",
    "    ### prepare an empty array\n",
    "    noise = np.zeros(shape=(1, LATENT_DIM))\n",
    "    encoded_text = np.zeros(shape=(1, EMBEDDING_DIM))\n",
    "\n",
    "    ### generate sample for input data\n",
    "    encoded_text[0, :] = vectorize_sentence(text)\n",
    "    noise[0, :] = np.random.uniform(0, 1, LATENT_DIM)\n",
    "\n",
    "    ### predict and generate an image\n",
    "    generated_images = generator.predict([noise, encoded_text])\n",
    "    generated_image = generated_images[0]\n",
    "\n",
    "    if flag is True:\n",
    "        generated_image = generated_image * 127.5 + 127.5\n",
    "        return Image.fromarray(generated_image.astype(np.uint8))\n",
    "    elif flag is not True:\n",
    "        return generated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def combine_normalized_images(generated_images):\n",
    "    num = generated_images.shape[0]\n",
    "    width = int(math.sqrt(num))\n",
    "    height = int(math.ceil(float(num) / width))\n",
    "    shape = generated_images.shape[1:]\n",
    "    image = np.zeros(\n",
    "        (height * shape[0], width * shape[1], shape[2]), dtype=generated_images.dtype\n",
    "    )\n",
    "    for index, img in enumerate(generated_images):\n",
    "        i = int(index / width)\n",
    "        j = index % width\n",
    "        image[\n",
    "            i * shape[0] : (i + 1) * shape[0], j * shape[1] : (j + 1) * shape[1], :\n",
    "        ] = img\n",
    "    return image\n",
    "\n",
    "\n",
    "def img_from_normalized_img(normalized_img):\n",
    "    image = normalized_img * 127.5 + 127.5\n",
    "    return Image.fromarray(image.astype(np.uint8))\n",
    "\n",
    "\n",
    "def generate_mode():\n",
    "    img_size = (64, 64, 3)\n",
    "    img_path = \"./emoji/edited/emoji_64x64/\"\n",
    "    txt_path = \"./emoji/description/detailed\"\n",
    "    glove_path = \"./utils/glove.6B.300d.txt\"\n",
    "\n",
    "    load_model()\n",
    "\n",
    "    iteration = 0\n",
    "    caption_list = []\n",
    "    print(\"Generating images...\")\n",
    "    for image, caption in zip(X_train, Captions):\n",
    "        edited_image = image * 127.5 + 127.5\n",
    "        edited_image = Image.fromarray(edited_image.astype(np.uint8))\n",
    "        edited_image.save(\"./images/original/\" + str(iteration) + \".png\")\n",
    "        generated_image = generate_image_from_text(caption)\n",
    "        generated_image.save(\"./images/output/\" + str(iteration) + \".png\")\n",
    "        caption_list.append([str(caption)])\n",
    "        iteration += 1\n",
    "\n",
    "    df = pd.DataFrame(caption_list, columns=[\"caption\"])\n",
    "    df.to_csv(\"./images/caption.csv\")\n",
    "\n",
    "    # plot all emojis\n",
    "    save_imgs(epoch=5000, texts=Captions, batch_size=260)\n",
    "    print(\"Done!\")\n",
    "\n",
    "\n",
    "def train_mode():\n",
    "    img_path = \"./emoji/edited/emoji_64x64/\"\n",
    "    txt_path = \"./emoji/description/detailed\"\n",
    "    glove_path = \"./utils/glove.6B.300d.txt\"\n",
    "\n",
    "    train(epochs=EPOCHS, batch_size=BATCH_SIZE, save_interval=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
