{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import shutil\n",
    "from pathlib import Path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def preprocess_emoji(dset_path):\n",
    "    # make emoji unicode vocabulary \n",
    "    code_vocaburary = {}\n",
    "    code_path = Path('dataset/description/unicode.txt')\n",
    "    code_list = code_path.read_text(encoding='utf-8').split('\\n')\n",
    "    for index, data in enumerate(code_list):\n",
    "        code_vocaburary[data] = index\n",
    "    \n",
    "    # chack dataset path\n",
    "    image_path = Path(dset_path)\n",
    "    if image_path.exists() == False:\n",
    "        exit('Check your dataset path!')\n",
    "\n",
    "    # copy designated emoji images\n",
    "    for filepath in list(image_path.glob(\"./**/64/**/*.png\")):\n",
    "        if str(filepath.name.split(\".\")[0]) in code_list:\n",
    "            shutil.copyfile(filepath, \\\n",
    "                'dataset/edited/' + str(code_vocaburary[filepath.name.split(\".\")[0]]) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started pre-processing\n",
      "Completed pre-processing\n"
     ]
    }
   ],
   "source": [
    "print(\"Started pre-processing\")\n",
    "preprocess_emoji(\"dataset/original\")\n",
    "print(\"Completed pre-processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SHAPE = (64, 64, 3)\n",
    "EMBEDDING_DIM = 300\n",
    "IMAGE_DIR = \"dataset/edited/\"\n",
    "TXT_DIR = \"dataset/description/detailed/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def load_dataset(img_dir, txt_dir, img_shape, split_rate = 0.1):\n",
    "    t_path = Path(txt_dir)\n",
    "    i_path = Path(img_dir)\n",
    "    \n",
    "    images = dict()\n",
    "    texts = dict()\n",
    "    \n",
    "    for filename in list(i_path.glob(\"*.png\")):\n",
    "        name = filename.name.replace('.png', '')\n",
    "        images[name] = filename.resolve()\n",
    "    \n",
    "    for filename in list(t_path.glob(\"*.txt\")):\n",
    "        name = filename.name.replace('.txt', '')\n",
    "        texts[name] = filename.read_text(encoding='utf-8').lower()\n",
    "\n",
    "    image_list = []\n",
    "    caption_list = []\n",
    "    numbers = []\n",
    "    \n",
    "    for name, item_path in images.items():\n",
    "        if name in texts:\n",
    "            text = texts[name]\n",
    "            text = text.replace(\"“\", \"\") # need to remove explicitly as ascii has only one double-quotes, no start-end double-quotes\n",
    "            text = text.replace(\"”\", \"\")\n",
    "            tokenized = sent_tokenize(text) # tokenizes sentences, delimiter = \".\"\n",
    "            label_number = int(name)\n",
    "\n",
    "            for sentence in tokenized:\n",
    "                regex_any_symbol = re.compile(\"[!-/:-@[-`{-~]\")\n",
    "                filtered_sentence = re.sub(regex_any_symbol, \"\", sentence) # removes any symbol from description\n",
    "#                 print(filtered_sentence)\n",
    "                image = img_to_array(load_img(item_path, target_size=(img_shape[0], img_shape[1])))\n",
    "                image = (image.astype(np.float32) / 127.5) - 1.\n",
    "#                 print(image)\n",
    "                image_list.append(image)\n",
    "                caption_list.append(filtered_sentence)\n",
    "                numbers.append(label_number)\n",
    "                \n",
    "    image_list = np.array(image_list)\n",
    "    caption_list = np.array(caption_list)\n",
    "    numbers = np.array(numbers)\n",
    "    \n",
    "    print('Dataset Size: %s' % len(image_list))\n",
    "    image_train, image_test, caption_train, caption_test, numbers_train, numbers_test = train_test_split(image_list, caption_list, numbers, test_size=split_rate)\n",
    "    \n",
    "    return image_train, caption_train, image_test, caption_test, numbers_train, numbers_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size: 260\n"
     ]
    }
   ],
   "source": [
    "image_train, caption_train, image_test, caption_test, numbers_train, numbers_test = \\\n",
    "    load_dataset(IMAGE_DIR,TXT_DIR, IMAGE_SHAPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "import zipfile\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def load_glove(glove_file_path, embedding_dim = EMBEDDING_DIM):\n",
    "    print(\"Loading glove file, please wait...\")\n",
    "    _word2em = {}\n",
    "    file = open(glove_file_path, mode='rt', encoding='utf8')\n",
    "    for line in file:\n",
    "        words = line.strip().split()\n",
    "        word = words[0]\n",
    "        embeds = np.array(words[1:], dtype=np.float32)\n",
    "        _word2em[word] = embeds\n",
    "    file.close()\n",
    "    print(\"Finished.\")\n",
    "    return _word2em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading glove file, please wait...\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "word2em = load_glove(\"/home/kaustubh/emotigan/utils/glove.6B.300d.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence2Em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def vectorize_sentence(sentence, embedding_dim = EMBEDDING_DIM):\n",
    "    words = sentence.split(\" \")\n",
    "    em = np.zeros(shape=(embedding_dim, ))\n",
    "    for word in words:\n",
    "        try:\n",
    "            em = np.add(em, word2em[word])\n",
    "        except KeyError:\n",
    "            print(\"Error: Not found \\\"\"+word+\"\\\"\")\n",
    "    return em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing\n",
    "len(vectorize_sentence(caption_test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu_options {\n",
      "  allow_growth: true\n",
      "  visible_device_list: \"0\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GPU setting\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.backend import set_session\n",
    "\n",
    "config = tf.ConfigProto(\n",
    "            gpu_options = tf.GPUOptions(\n",
    "                visible_device_list=\"0\", # specify GPU number\n",
    "                allow_growth=True)\n",
    "        )\n",
    "\n",
    "print(config)\n",
    "\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, concatenate\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as kb\n",
    "from keras.layers import Lambda\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(latent_dim = 100, embedding_dim = EMBEDDING_DIM):\n",
    "    generator_input = Input(shape=(self.latent_dim, ), name=\"g_input\")\n",
    "    cond_input = Input(shape=(self.embedding_dim, ), name=\"cond_g_input\")\n",
    "    cond_output = Dense(100)(cond_input)\n",
    "\n",
    "    G = concatenate([generator_input, cond_output])\n",
    "    G = Dense(256 * 8 * 8, activation=\"relu\")(G)\n",
    "    G = Reshape((8, 8, 256))(G)\n",
    "    G = UpSampling2D()(G)\n",
    "    G = Conv2D(256, kernel_size=3, padding=\"same\")(G)\n",
    "    G = BatchNormalization(momentum=0.8)(G)\n",
    "    G = Activation(\"relu\")(G)\n",
    "    G = UpSampling2D()(G)\n",
    "    G = Conv2D(128, kernel_size=3, padding=\"same\")(G)\n",
    "    G = BatchNormalization(momentum=0.8)(G)\n",
    "    G = Activation(\"relu\")(G)\n",
    "    G = UpSampling2D()(G)\n",
    "    G = Conv2D(64, kernel_size=3, padding=\"same\")(G)\n",
    "    G = BatchNormalization(momentum=0.8)(G)\n",
    "    G = Activation(\"relu\")(G)\n",
    "    G = Conv2D(self.channels, kernel_size=3, padding=\"same\")(G)\n",
    "    generator_output = Activation(\"tanh\")(G)\n",
    "\n",
    "    generator = Model([generator_input, cond_input], generator_output)\n",
    "    generator.summary()\n",
    "\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(img_shape = IMAGE_SHAPE, embedding_dim = EMBEDDING_DIM):\n",
    "    discriminator_input = Input(shape=self.img_shape, name=\"d_input\")\n",
    "    cond_input = Input(shape=(self.embedding_dim, ), name=\"cond_d_input\")\n",
    "    D = Conv2D(64, kernel_size=3, strides=2, padding=\"same\")(discriminator_input)\n",
    "    D = LeakyReLU(alpha=0.2)(D)\n",
    "    D = Dropout(0.25)(D)\n",
    "    D = Conv2D(128, kernel_size=3, strides=2, padding=\"same\")(D)\n",
    "    D = ZeroPadding2D(padding=((0,1),(0,1)))(D)\n",
    "    D = BatchNormalization(momentum=0.8)(D)\n",
    "    D = LeakyReLU(alpha=0.2)(D)\n",
    "    D = Dropout(0.25)(D)\n",
    "    D = Conv2D(256, kernel_size=3, strides=1, padding=\"same\")(D)\n",
    "    D = BatchNormalization(momentum=0.8)(D)\n",
    "    D = LeakyReLU(alpha=0.2)(D)\n",
    "    D = Dropout(0.25)(D)\n",
    "    D = Conv2D(512, kernel_size=3, strides=2, padding=\"same\")(D)\n",
    "    D = BatchNormalization(momentum=0.8)(D)\n",
    "    D = LeakyReLU(alpha=0.2)(D)\n",
    "\n",
    "    cond_d_hidden = Dense(100)(cond_input)\n",
    "    cond_d_hidden = Reshape((1, 1, 100))(cond_d_hidden)\n",
    "    cond_d_output = Lambda(lambda x: kb.tile(x, [1, 9, 9, 1]))(cond_d_hidden)\n",
    "\n",
    "    D = concatenate([D, cond_d_output], axis=-1)\n",
    "    D = Conv2D(512, kernel_size=3, strides=1, padding='same')(D)\n",
    "    D = BatchNormalization(momentum=0.8)(D)\n",
    "    D = LeakyReLU(alpha=0.1)(D)\n",
    "    D = Dropout(0.25)(D)\n",
    "    D = Flatten()(D)\n",
    "    discriminator_output = Dense(1, activation='sigmoid')(D)\n",
    "\n",
    "    discriminator = Model([discriminator_input, cond_input], discriminator_output)\n",
    "    discriminator.summary()\n",
    "\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
